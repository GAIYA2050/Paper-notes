### 1.为什么说 Dropout 可以解决过拟合？
（1）取平均的作用： “综合起来取平均” 的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些 “相反的” 拟合互相抵消。dropout 掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个 dropout 过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为 “反向” 的拟合相互抵消就可以达到整体上减少过拟合。

（2）减少神经元之间复杂的共适应关系： 因为 dropout 程序导致两个神经元不一定每次都在一个 dropout 网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看 dropout 就有点像 L1，L2 正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。  

### 2.Dropout 与 L1 和 L2 正则化有什么区别？
L1 和 L2 正则化修改代价函数，Dropout 修改神经网络本身。  

### 3.评价指标有哪些？
机器学习中评价指标： Accuracy（准确率）、 Precision（查准率或者精准率）、Recall（查全率或者召回率）。

目标检测的指标：识别精度，识别速度，定位精度。

a、目标检测中衡量识别精度的指标是 mAP（mean average precision）。多个类别物体检测中，每一个类别都可以根据 recall 和 precision 绘制一条曲线，AP 就是该曲线下的面积，mAP 是多个类别 AP 的平均值。

b、  目标检测评价体系中衡量定位精度的指标是 IoU,IoU 就是算法预测的目标窗口和真实的目标窗口的交叠（两个窗口面积上的交集和并集比值）

关键点检测的评价指标，nme平均误差，failure rate小于指定nme的样本的比例

### 4.非极大值抑制算法步骤
非极大值抑制算法（Non-maximum suppression，NMS）在目标检测中经常用到。我们的检测算法可能对同一目标产生多次检测的结果，非极大值抑制算法可以保证每个目标只检测一次，找到检测效果最好的框。
 1. 选出概率最大的框为标准。

 2. 依次计算其他框和选出的框的相交面积，记做IOU，当IOU>某一阀值时，删除该框。直至没有IOU>某一阀值的框存在为止。然后保留选出的框

 3. 对于存在除了上述操作选出框之外的其他框继续重复1和2的操作。

 4. 整个过程直到所有剩余的框都是经过1和2操作进行之后保留下来的为止

若检测物体是多个类别的时候，一般情况是对每个类别分别做一次非极大值抑制算法。

### 5.斜着的矩形框如何求 iou？两个多边形的框如何求 iou？
首先要求解两个多边形的面积

关键在于如何求出交集的面积

思路一：

蒙特卡洛 + 采样，近似求解交集的面积，但是中间涉及判断点在不在多边形内，判断点是否在多边形内

思路二：

适合于两个凸多边形，凸多边形可以看做是半平面的交集，因此两个凸多边形的交集，可以看作是（m+n）个半平面的交集（假设两个凸多边形分别有 m 个顶点和 n 个顶点），求出来半平面的交集（仍旧是一个凸多边形）之后，求解该多边形的面积即可。求解半平面交集  

### 6.RPN 的平移不变形如何解决？
在计算机视觉中的一个挑战就是平移不变性：比如人脸识别任务中，小的人脸 (24*24 的分辨率) 和大的人脸 (1080*720) 如何在同一个训练好权值的网络中都能正确识别。若是平移了图像中的目标，则建议框也应该平移，也应该能用同样的函数预测建议框。

传统有两种主流的解决方式：

第一、对图像或 feature map 层进行尺度 \ 宽高的采样；

第二、对滤波器进行尺度 \ 宽高的采样 (或可以认为是滑动窗口).

但 Faster R-CNN 解决该问题的具体实现是：通过卷积核中心 (用来生成推荐窗口的 Anchor) 进行尺度、宽高比的采样，使用 3 种尺度和 3 种比例来产生 9 种 anchor。  

### 7.RPN 生成候选框样本不均衡有什么问题？
1. 为了训练速度和训练精度的平衡，原始图像进入训练之前需要先进行 resize，使图像的短边为 600（或者长边为 1000）； 

2. 在训练过程产生的 anchor 中，忽视掉所有的超过图像边界的 anchor：如在 1000*600 的图像中，大概会有 20000（60*40*9）个 anchor，去除掉超过边界的 anchor 之后，还剩 6000 个。论文中提到：若是不去除这些 anchor 的话，它们会带来大量的、难以纠正的错误，并且导致训练 loss 难以收敛。而在测试过程中，对于超出边界的 anchor 并不是删除，而是修剪掉其超过边界的部分。 

3. RPN 得到的大量 proposal 可能会相互重叠，冗余度较高，论文根据这些 proposal 的 cls 得分对其区域采用非极大值抑制（NMS）去除冗余 proposal，经过 NMS 后每张图还剩下大概 2000 个 proposal。经过实验，NMS 并不会降低检测的准确度，但是大量减少了无关的 proposal。

4. 对每个标定的 ground true box 区域，与其重叠比例最大的 anchor 记为正样本 (保证每个 ground true 至少对应一个正样本 anchor)

   对 1 中剩余的 anchor，如果其与某个标定区域重叠比例 (IoU) 大于 0.7，记为正样本（每个 ground true box 可能会对应多个正样本 anchor。但每个正样本 anchor 只可能对应一个 grand true box）；如果其与任意一个标定的重叠比例都小于 0.3，记为负样本。

   对 1、2 剩余的 anchor，弃去不用。

   跨越图像边界的 anchor 弃去不用。  
   
 ### 8.池化主要有哪几种？
（1）一般池化（General Pooling）

其中最常见的池化操作有平均池化、最大池化：

平均池化（average pooling）：计算图像区域的平均值作为该区域池化后的值。

最大池化（max pooling）：选图像区域的最大值作为该区域池化后的值。

（2）重叠池化（OverlappingPooling）

重叠池化就是，相邻池化窗口之间有重叠区域，此时一般 sizeX > stride。

（3）空金字塔池化（Spatial Pyramid Pooling）

空间金字塔池化的思想源自 Spatial Pyramid Model，它将一个 pooling 变成了多个 scale 的 pooling。用不同大小池化窗口作用于上层的卷积特征。也就是说 spatital pyramid pooling layer 就是把前一卷积层的 feature maps 的每一个图片上进行了 3 个卷积操作，并把结果输出给全连接层。其中每一个 pool 操作可以看成是一个空间金字塔的一层。（具体的细节可以看下面的参考链接，讲的比较详细）

这样做的好处是，空间金字塔池化可以把任意尺度的图像的卷积特征转化成相同维度，这不仅可以让 CNN 处理任意尺度的图像，还能避免 cropping 和 warping 操作，导致一些信息的丢失，具有非常重要的意义  

### 9.池化层主要的作用
（1）首要作用，下采样（downsamping）

（2）降维、去除冗余信息、对特征进行压缩、简化网络复杂度、减小计算量、减小内存消耗等等。各种说辞吧，总的理解就是减少参数量。

（3）实现非线性（这个可以想一下，relu 函数，是不是有点类似的感觉？）。

（4）可以扩大感知野。

（5）可以实现不变性，其中不变形性包括，平移不变性、旋转不变性和尺度不变性。

A：特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的 resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。

B：特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。

### 10.如何应对图像光照变化大？
1、直方图均衡化

2、对比度拉伸，或者调节

3、若受光源影响，使得图片整体色彩往一方面移动，用白平衡算法进行修正，使其发黄、发蓝、发红的照片更加趋于自然光下的图像

4、若是过爆或者过暗，可是设计阈值函数，不用全局阈值，对特定区域进行特定阈值分割。

5、若是太暗，可以采用对数变化，对数图像增强是图像增强的一种常见方法，其公式为： S = c log (r+1)，对数使亮度比较低的像素转换成亮度比较高的，而亮度较高的像素则几乎没有变化，这样就使图片整体变亮。

6、采用拉普拉斯算子增强 ， filter2D (src,dst)  

### 11.为什么要使用许多小卷积核 (如 3 x 3) 而不是几个大卷积核？
首先，可以使用几个较小的核而不是几个较大的核来获得相同的感受野并捕获更多的空间上下文，但是使用较小的内核时，使用的参数和计算量较少。  
其次，因为使用更小的核，将使用更多的滤波器，将能够使用更多的激活函数，从而使 CNN 学习到更具区分性的映射函数。

### 12.梯度消失与梯度爆炸的产生原因是什么？
梯度消失：

（1）隐藏层的层数过多；

（2）采用了不合适的激活函数 (更容易产生梯度消失，但是也有可能产生梯度爆炸)

梯度爆炸：

（1）隐藏层的层数过多；

（2）权重的初始化值过大

### 13.如何确定是否出现梯度爆炸？
1.模型无法从训练数据中获得更新（如低损失）。

2.模型不稳定，导致更新过程中的损失出现显著变化。

3.训练过程中，模型损失变成 NaN。

### 14.如何解决梯度爆炸与梯度消失？
1、预训练加微调 

2、梯度剪切、权重正则化（针对梯度爆炸） 

3、用 ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout 等替代 sigmoid 函数。

4、使用 batchnorm

5、使用残差结构

6、LSTM 的结构设计也可以改善 RNN 中的梯度消失问题。

### 15.有哪些提高卷积神经网络运算效率的方法？
1、设计新的卷积神经网络训练策略

目前卷积神经网络的训练是利用 BP 算法以整体监督方式进行学习的，通过采用某种方式的预训练（监督式或无监督式），可以为卷积神经网络提供一个较好的初始值，从而大大提高其整体上使用 BP 算法进行训练的收敛速度。

2、使用 GPU 加速卷积运算过程

基于 GPU 运算，通过撰写高效 C++ 代码实现卷积操作，可以将卷积操作速度提高 3~10 倍。

3、使用并行计算提高网络训练和测试速度

将一个大的卷积神经网络划分为几个小的子网络，再并行处理每个子网络中的运算过程，可以有效改善整个大网络的运算过程。

4、采用分布式计算提高网络训练和测试速度

该加速方式使用成千上万个运算节点，每个运算节点值完成整个网络计算中的很小一部分计算，通过调度节点为每个运算节点分配相应的计算任务，每个运算节点分别完成各自的计算任务，所有计算节点的计算任务都完成之后，调度节点再将每个计算节点的计算结果汇总融合。

采用该加速方式，速度提高非常明显，可以完成一些以前几乎不能完成的网络训练任务，但主要问题是相关程序编制较为复杂，需要额外消耗较多的计算资源。

5、硬件化卷积神经网络

卷积神经网络成功应用于越来越多的实际问题之中，卷积神经网络中的卷积层和下采样层如果能够被硬件化，将会再次提高卷积神经网络的运行效率。

### 16.简述 CNN 的演变
·LeNet：2 个卷积 3 个全连接，最早用于数字识别

·AlexNet：12 年 ImageNet 冠军，5 个卷积 3 个全连接，多个小卷积代替单一大卷积；使用 ReLU 激活函数，解决梯度小数问题；引入 dropout 避免模型过拟合；最大池化。

·ZF-Net：13 年 ImageNet 冠军，只用了一块 GPU 的稠密连接结构；将 AlexNet 第一层卷积核由 11 变成 7，步长由 4 变为 2。

·VGG-Nets：14 年 ImageNet 分类第二名，更深的网络，卷积层使用更小的 filter 尺寸和间隔；多个小卷积让网络有更多的非线性，更少的参数。

·GoogLeNet：14 年 ImageNet 分类第一名。引入 Inception 模块，采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合；采用了 average pooling 来代替全连接层；避免梯度消失，网络额外增加了 2 个辅助的 softmax 用于向前传导梯度。

·ResNet：引入残差单元，简化学习目标和难度，加快训练速度，模型加深时，不会产生退化问题；能够有效解决训练过程中梯度消失和梯度爆炸问题。

·DenseNet：密集连接；加强特征传播，鼓励特征复用，极大的减少了参数量。

### 17.讲一下 CNN，每个层，及作用
·卷积层（Conv）：使用卷积核进行特征提取和特征映射

·激活函数（Activation）：由于卷积也是一种线性运算，因此需要增加非线性映射

·池化层（Pool）：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征

·全连接层（FC）：连接所有的特征，将输出值送给分类器

### 18.卷积神经网络中空洞卷积的作用是什么？
空洞卷积也叫扩张卷积，在保持参数个数不变的情况下增大了卷积核的感受野，同时它可以保证输出的特征映射（feature map）的大小保持不变。一个扩张率为 2 的 3×3 卷积核，感受野与 5×5 的卷积核相同，但参数数量仅为 9 个。

### 19.目标检测里如何有效解决常见的前景少背景多的问题
·采用 Focal Loss 或 OHEM 进行负样本挖掘，加大 Hard Example 损失权重

·训练时只利用 Ground Truth 周边的 Prior Boxes 进行训练，忽略其他背景区域，只考虑困难背景区域

### 20.目标检测主要的难点有哪些？
1.检测速度：实时性要求高，故网络结构不能太复杂，参数不能太多，卷积层次也不能太多。

2.位置准确率：（x y w h）参数必须准确，也就是检测框大小尺寸要匹配，且重合度 IOU 要高。SSD 和 faster RCNN 通过多个 bounding box 来优化这个问题

3.漏检率：必须尽量检测出所有目标物体，特别是靠的近的物体和尺寸小的物体。SSD 和 faster RCNN 通过多个 bounding box 来优化这个问题

4.物体宽高比例不常见：SSD 通过不同尺寸 feature map，yoloV2 通过不同尺寸输入图片，来优化这个问题。

5.靠的近的物体准确率低

6.小尺寸物体准确率低：SSD 取消全连接层，yoloV2 增加 pass through layer，采用高分辨率输入图片，来优化这个问题

### 21.简述 Inception v1-v4 区别、改进
**V1**

采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合；

将 CNN 中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性；

为了减少计算量，增加了 1x1 卷积。

 **V2**

卷积分解，将单个的 5x5 卷积层用 2 个连续的 3x3 卷积层组成的小网络来代替，在保持感受野范围的同时又减少了参数量，也加深了网络。

提出了著名的 Batch Normalization (BN) 方法。BN 会对每一个 mini-batch 数据的内部进行标准化（normalization）, 使输出规范到 N（0，1）的正态分布，加快了网络的训练速度，还可以增大学习率。

BN 某种意义上起到了正则化的作用，所以可以减少或者取消 dropout，简化网络结构。V2 在训练达到 V1 准确率时快了 14 倍，最后收敛的准确率也比 V1 高。

 **V3**

考虑了 nx1 卷积核，将一个较大的二维卷积拆成两个较小的一维卷积（7x7 拆成了 7x1 和 1x7，3x3 拆成了 1x3 和 3x1），一方面节约了大量参数，加速运算并减轻了过拟合），同时网络深度进一步增加，增加了网络的非线性。

优化了 Inception Module 的结构。

**V4**

利用残差连接（Residual Connection）来改进 V3 结构。

### 22.为什么深度学习中的模型基本用3x3和5x5的卷积（奇数），而不是2x2和4x4的卷积（偶数）？
使用 2x2 的卷积想要获得较大的感受野，需要更多的网络层次，比如想要获得 5x5 的感受野，可以使用两层 3x3 的卷积，如果使用 2x2 的卷积则需要四层。越深的网络越容易梯度消息，越难训练，使用 2x2 的卷积会在一定程度上增加网络训练的难度。

### 23.1x1 卷积作用
1. 实现跨通道的交互和信息整合

2. 进行卷积核通道数的降维和升维

3、实现多个 feature map 的线性组合，实现通道个数的变换。

4、对特征图进行一个比例缩放

### 24.深度学习为什么要这么深？
1、一个直观的解释，从模型复杂度角度。如果我们能够增强一个学习模型的复杂度，那么它的学习能力能够提升。如何增加神经网络的复杂度呢？要么变宽，即增加隐层网络神经元的个数；要么变深，即增加隐层的层数。当变宽的时候，只不过是增加了一些计算单元，增加了函数的个数，在变深的时候不仅增加了个数，还增加了函数间的嵌入的程度。

2、深度学习可以通过多个 layer 的转换学习更高维度的特征来解决更加复杂的任务。

3、那现在我们为什么可以用这样的模型？有很多因素，第一我们有了更大的数据；第二我们有强力的计算设备；第三我们有很多有效的训练技巧

4、像在 ZFNet 网络中已经体现，特征间存在层次性，层次更深，特征不变性强，类别分类能力越强，要学习复杂的任务需要更深的网络

### 25.神经网络怎样进行参数初始化？
1. 所有的参数初始化为 0 或者相同的常数

2. 随机初始化

3. Batch Normalization

4. Xavier

5. MSRA 

参数初始化的理想状态是参数正负各半，期望为 0。

如果权值的初始值过大，则会导致梯度爆炸，使得网络不收敛；过小的权值初始值，则会导致梯度消失，会导致网络收敛缓慢或者收敛到局部极小值。

正确的初始化方法应该避免指数级地减小或放大输入值的大小，防止梯度 “饱和”。 Glorot 提出两个准则：

    各个层激活值的方差保持不变（正向传播）

    各个层的梯度值的方差保持不变（反向传播）

通常初始的权值矩阵的均值为 0.

这这些条件的基础上，Glorot 使用 (tanh) 作为激活函数，并假设输入值的均值为 0，提出了 Xavier 初始化的方法。

而 Kaiming 使用 ReLU 作为激活函数，就无法满足数值的均值为 0 的条件，因此使用 Xavier 来初始化 ReLU 作为激活函数的网络，效果也就不是那么理想。其提出了 MSRA 的初始化方法，来解决该问题，


    各个层激活值梯度的方差保持不变（反向传播）

    各个层的值的方差保持不变（正向传播）
### 26.为什么引入 Relu 呢？
第一，采用 sigmoid 等函数，反向传播求误差梯度时，求导计算量很大，而 Relu 求导非常容易。

第二，对于深层网络，sigmoid 函数反向传播时，很容易就会出现梯度消失的情况（在 sigmoid 接近饱和区时，变换太缓慢，导数趋于 0），从而无法完成深层网络的训练。

第三，Relu 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生
