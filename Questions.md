### 1.为什么说 Dropout 可以解决过拟合？
（1）取平均的作用： “综合起来取平均” 的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些 “相反的” 拟合互相抵消。dropout 掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个 dropout 过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为 “反向” 的拟合相互抵消就可以达到整体上减少过拟合。

（2）减少神经元之间复杂的共适应关系： 因为 dropout 程序导致两个神经元不一定每次都在一个 dropout 网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看 dropout 就有点像 L1，L2 正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。  

### 2.Dropout 与 L1 和 L2 正则化有什么区别？
L1 和 L2 正则化修改代价函数，Dropout 修改神经网络本身。  

### 3.评价指标有哪些？
机器学习中评价指标： Accuracy（准确率）、 Precision（查准率或者精准率）、Recall（查全率或者召回率）。

目标检测的指标：识别精度，识别速度，定位精度。

a、目标检测中衡量识别精度的指标是 mAP（mean average precision）。多个类别物体检测中，每一个类别都可以根据 recall 和 precision 绘制一条曲线，AP 就是该曲线下的面积，mAP 是多个类别 AP 的平均值。

b、  目标检测评价体系中衡量定位精度的指标是 IoU,IoU 就是算法预测的目标窗口和真实的目标窗口的交叠（两个窗口面积上的交集和并集比值）

关键点检测的评价指标，nme平均误差，failure rate小于指定nme的样本的比例

### 4.非极大值抑制算法步骤
非极大值抑制算法（Non-maximum suppression，NMS）在目标检测中经常用到。我们的检测算法可能对同一目标产生多次检测的结果，非极大值抑制算法可以保证每个目标只检测一次，找到检测效果最好的框。
 1. 选出概率最大的框为标准。

 2. 依次计算其他框和选出的框的相交面积，记做IOU，当IOU>某一阀值时，删除该框。直至没有IOU>某一阀值的框存在为止。然后保留选出的框

 3. 对于存在除了上述操作选出框之外的其他框继续重复1和2的操作。

 4. 整个过程直到所有剩余的框都是经过1和2操作进行之后保留下来的为止

若检测物体是多个类别的时候，一般情况是对每个类别分别做一次非极大值抑制算法。

### 5.斜着的矩形框如何求 iou？两个多边形的框如何求 iou？
首先要求解两个多边形的面积

关键在于如何求出交集的面积

思路一：

蒙特卡洛 + 采样，近似求解交集的面积，但是中间涉及判断点在不在多边形内，判断点是否在多边形内

思路二：

适合于两个凸多边形，凸多边形可以看做是半平面的交集，因此两个凸多边形的交集，可以看作是（m+n）个半平面的交集（假设两个凸多边形分别有 m 个顶点和 n 个顶点），求出来半平面的交集（仍旧是一个凸多边形）之后，求解该多边形的面积即可。求解半平面交集  

### 6.RPN 的平移不变形如何解决？
在计算机视觉中的一个挑战就是平移不变性：比如人脸识别任务中，小的人脸 (24*24 的分辨率) 和大的人脸 (1080*720) 如何在同一个训练好权值的网络中都能正确识别。若是平移了图像中的目标，则建议框也应该平移，也应该能用同样的函数预测建议框。

传统有两种主流的解决方式：

第一、对图像或 feature map 层进行尺度 \ 宽高的采样；

第二、对滤波器进行尺度 \ 宽高的采样 (或可以认为是滑动窗口).

但 Faster R-CNN 解决该问题的具体实现是：通过卷积核中心 (用来生成推荐窗口的 Anchor) 进行尺度、宽高比的采样，使用 3 种尺度和 3 种比例来产生 9 种 anchor。  

### 7.RPN 生成候选框样本不均衡有什么问题？
1. 为了训练速度和训练精度的平衡，原始图像进入训练之前需要先进行 resize，使图像的短边为 600（或者长边为 1000）； 

2. 在训练过程产生的 anchor 中，忽视掉所有的超过图像边界的 anchor：如在 1000*600 的图像中，大概会有 20000（60*40*9）个 anchor，去除掉超过边界的 anchor 之后，还剩 6000 个。论文中提到：若是不去除这些 anchor 的话，它们会带来大量的、难以纠正的错误，并且导致训练 loss 难以收敛。而在测试过程中，对于超出边界的 anchor 并不是删除，而是修剪掉其超过边界的部分。 

3. RPN 得到的大量 proposal 可能会相互重叠，冗余度较高，论文根据这些 proposal 的 cls 得分对其区域采用非极大值抑制（NMS）去除冗余 proposal，经过 NMS 后每张图还剩下大概 2000 个 proposal。经过实验，NMS 并不会降低检测的准确度，但是大量减少了无关的 proposal。

4. 对每个标定的 ground true box 区域，与其重叠比例最大的 anchor 记为正样本 (保证每个 ground true 至少对应一个正样本 anchor)

   对 1 中剩余的 anchor，如果其与某个标定区域重叠比例 (IoU) 大于 0.7，记为正样本（每个 ground true box 可能会对应多个正样本 anchor。但每个正样本 anchor 只可能对应一个 grand true box）；如果其与任意一个标定的重叠比例都小于 0.3，记为负样本。

   对 1、2 剩余的 anchor，弃去不用。

   跨越图像边界的 anchor 弃去不用。  
   
 ### 8.池化主要有哪几种？
（1）一般池化（General Pooling）

其中最常见的池化操作有平均池化、最大池化：

平均池化（average pooling）：计算图像区域的平均值作为该区域池化后的值。

最大池化（max pooling）：选图像区域的最大值作为该区域池化后的值。

（2）重叠池化（OverlappingPooling）

重叠池化就是，相邻池化窗口之间有重叠区域，此时一般 sizeX > stride。

（3）空金字塔池化（Spatial Pyramid Pooling）

空间金字塔池化的思想源自 Spatial Pyramid Model，它将一个 pooling 变成了多个 scale 的 pooling。用不同大小池化窗口作用于上层的卷积特征。也就是说 spatital pyramid pooling layer 就是把前一卷积层的 feature maps 的每一个图片上进行了 3 个卷积操作，并把结果输出给全连接层。其中每一个 pool 操作可以看成是一个空间金字塔的一层。（具体的细节可以看下面的参考链接，讲的比较详细）

这样做的好处是，空间金字塔池化可以把任意尺度的图像的卷积特征转化成相同维度，这不仅可以让 CNN 处理任意尺度的图像，还能避免 cropping 和 warping 操作，导致一些信息的丢失，具有非常重要的意义  

### 9.池化层主要的作用
（1）首要作用，下采样（downsamping）

（2）降维、去除冗余信息、对特征进行压缩、简化网络复杂度、减小计算量、减小内存消耗等等。各种说辞吧，总的理解就是减少参数量。

（3）实现非线性（这个可以想一下，relu 函数，是不是有点类似的感觉？）。

（4）可以扩大感知野。

（5）可以实现不变性，其中不变形性包括，平移不变性、旋转不变性和尺度不变性。

A：特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的 resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。

B：特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。
